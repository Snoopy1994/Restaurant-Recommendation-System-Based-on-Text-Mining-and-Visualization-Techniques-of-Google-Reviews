{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953ef6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74bcb569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import default module\n",
    "import os\n",
    "import time\n",
    "import math \n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# import self module\n",
    "import utils\n",
    "import liwc_data\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996e1590",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d6fe80",
   "metadata": {},
   "source": [
    "### preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d39f1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionaries = [\n",
    "    '../liwc_dictionary/LIWC2015.txt',\n",
    "    '../liwc_dictionary/new_add_liwc_dic.txt',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35352175",
   "metadata": {},
   "outputs": [],
   "source": [
    "liwc_dic = liwc_data.LingusticsInquiry()\n",
    "liwc_dic.add_dictionaries(*dictionaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92f50f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a shorthand for loading a pickle file\n",
    "def read_pickle(path):\n",
    "    if path[-4:] != '.pkl':\n",
    "        raise ValueError\n",
    "        \n",
    "    with open(path, 'rb') as fd:\n",
    "        f = pickle.load(fd)\n",
    "    return f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69e32a1",
   "metadata": {},
   "source": [
    "### sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e634154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input data, dictionary with {ws, pos, ner}\n",
    "test_data = read_pickle('../../cleaned_data/taichung/word_segments/Akuan Hot Pot.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fdf7dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check_sets = {'posemo (Positive Emotions)', 'negemo (Negative Emotions)', 'negate (Negations)'}\n",
    "\n",
    "# for wss in test_data['ws']:\n",
    "#     for ws in wss:\n",
    "#         categories = get_categories(ws.strip())\n",
    "#         if len(categories.intersection(check_sets)) > 1:\n",
    "#             print (ws)\n",
    "#             print (categories.intersection(check_sets))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58a5b2e",
   "metadata": {},
   "source": [
    "#### scoring a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593409d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(categories):\n",
    "    # posemo (Positive Emotions) and negemo (Negative Emotions) can exist as the same time\n",
    "    _score_table = {\n",
    "        'posemo (Positive Emotions)': 1,\n",
    "        'negemo (Negative Emotions)': -1\n",
    "    }\n",
    "    \n",
    "    categories = set(categories)\n",
    "    scores = [_score_table.get(category, 0) for category in categories]\n",
    "    return 0 + sum(scores)\n",
    "\n",
    "# expand scores\n",
    "def inpaint_scores(nums):\n",
    "    # make [1, 0, 1] -> [?, True, ?]\n",
    "    return utils.sliding_mean(np.abs(nums), 1) > 0.6\n",
    "\n",
    "# scores, sentence splits, segment splits(including sentence splits)\n",
    "def get_segment_score(scores):\n",
    "    if np.all(scores == 0):\n",
    "        return 0\n",
    "    \n",
    "    scores[scores==0] = 1\n",
    "    return np.prod(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba8b7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "words  = np.r_[test_data['ws'][0]]\n",
    "categories = [liwc_dic.get_categories(word) for word in words]\n",
    "scores = np.r_[[get_score(category) for category in categories]]\n",
    "\n",
    "print (np.vstack([words, scores]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4036c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assume a string containing space only is a delimiter of sentences\n",
    "sentence_masks = np.r_[[len(word.strip()) == 0 for word in words]]\n",
    "sentence_splits = np.where(sentence_masks)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25aaa96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = 'negate (Negations)'\n",
    "negate_masks = np.r_[[key in category for category in categories]]\n",
    "\n",
    "# make both value is zero and is negate to -1\n",
    "scores[(scores == 0) & negate_masks] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9aced54",
   "metadata": {},
   "outputs": [],
   "source": [
    "inpaint_score_masks = [inpaint_scores(i) for i in np.split(scores, sentence_splits) if len(i) != 0]\n",
    "inpaint_score_masks = np.hstack(inpaint_score_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221bbc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = negate_masks | (scores != 0) | inpaint_score_masks\n",
    "score_splits = utils.label_change(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45737cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = np.hstack([sentence_splits, score_splits])\n",
    "splits = np.unique(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8544408",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# the sample result\n",
    "arr = np.vstack([words, scores]).T\n",
    "np.split(arr, splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e590ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_score = [get_segment_score(segment_scores) for segment_scores in np.split(scores, splits)]\n",
    "review_score = np.sum(review_score)\n",
    "review_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243b4f0e",
   "metadata": {},
   "source": [
    "#### scoring all sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1715380",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_review_scores(words, return_details=False):\n",
    "    \n",
    "    categories = [liwc_dic.get_categories(word) for word in words]\n",
    "    scores = np.r_[[get_score(category) for category in categories]]\n",
    "    \n",
    "    # assume a string containing space only is a delimiter of sentences\n",
    "    sentence_masks = np.r_[[len(word.strip()) == 0 for word in words]]\n",
    "    sentence_splits = np.where(sentence_masks)[0]\n",
    "    \n",
    "    key = 'negate (Negations)'\n",
    "    negate_masks = np.r_[[key in category for category in categories]]\n",
    "\n",
    "    # make both value is zero and is negate to -1\n",
    "    scores[(scores == 0) & negate_masks] = -1\n",
    "    \n",
    "    inpaint_score_masks = [\n",
    "        inpaint_scores(i) for i in np.split(scores, sentence_splits) if len(i) != 0]\n",
    "    inpaint_score_masks = np.hstack(inpaint_score_masks)\n",
    "    \n",
    "    labels = negate_masks | (scores != 0) | inpaint_score_masks\n",
    "    score_splits = utils.label_change(labels)\n",
    "    \n",
    "    splits = np.hstack([sentence_splits, score_splits])\n",
    "    splits = np.unique(splits)\n",
    "    \n",
    "    review_scores = [\n",
    "        get_segment_score(segment_scores)\n",
    "        for segment_scores in np.split(scores, splits)]\n",
    "    \n",
    "    if return_details:\n",
    "        arr = np.vstack([words, scores]).T\n",
    "        return review_scores, (arr, sentence_splits, splits)\n",
    "\n",
    "    return review_scores\n",
    "\n",
    "def get_review_score(words, return_details=False):\n",
    "    scores, details = get_review_scores(words, return_details=True)\n",
    "    score = np.sum(scores)\n",
    "    if return_details:\n",
    "        return score, details\n",
    "    return score\n",
    "\n",
    "def review_scores_based_on_sentence(words):\n",
    "    scores, (arr, sentence_splits, segment_splits) = \\\n",
    "        get_review_scores(words, return_details=True)\n",
    "    I = np.searchsorted(segment_splits, sentence_splits) + 1\n",
    "    sentence_scores = [np.sum(i) for i in np.split(scores, I)]\n",
    "    sentence_scores = [min(max(i, -1), 1) for i in sentence_scores]\n",
    "    return sentence_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b5a200",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = arr[:, 1]\n",
    "scores = np.split(score, splits)\n",
    "scores = [s.astype(np.int64) for s in scores]\n",
    "len(np.nonzero(np.array([get_segment_score(s) for s in scores]) != 0)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bd1982",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print ([get_review_score(words) for words in test_data['ws']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b809f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "print ([np.sum(review_scores_based_on_sentence(words)) for words in test_data['ws']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23c3bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "score, details = get_review_scores(test_data['ws'][121], return_details=True)\n",
    "print (score)\n",
    "arr, splits, seg_splits = details\n",
    "np.split(arr, splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a22224a",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = arr[:, 1]\n",
    "scores = np.split(score, splits)\n",
    "scores = [s.astype(np.int64) for s in scores]\n",
    "len(np.nonzero(np.array([get_segment_score(s) for s in scores]) != 0)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7e7e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = review_scores_based_on_sentence(test_data['ws'][0])\n",
    "len(np.nonzero(np.array(arr) != 0)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4b2f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_to_star(numerator, denominator):\n",
    "    if numerator == 0:\n",
    "        return 2 * numerator + 3\n",
    "    return 2 * numerator / denominator + 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da9b5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check(reviews):\n",
    "#     reviews = test_data['ws'][0]\n",
    "\n",
    "    # get estimated star\n",
    "    sentence_scores = review_scores_based_on_sentence(reviews)\n",
    "    numerator = np.sum(sentence_scores)\n",
    "    denominator = len(np.nonzero(sentence_scores)[0])\n",
    "    estimated_star = score_to_star(numerator, denominator)\n",
    "\n",
    "    def f(segment_arr):\n",
    "        x = ['(%s, %s)' % (i, j) for i, j in segment_arr]\n",
    "        x = ', '.join(x)\n",
    "    #     x = '[' + x + ']'\n",
    "        return x\n",
    "\n",
    "    segment_scores, (arr, sentence_splits, segment_splits) = \\\n",
    "        get_review_scores(reviews, return_details=True)\n",
    "\n",
    "    sentence_info = [f(i) for i in np.split(arr, sentence_splits)]\n",
    "    return estimated_star, list(zip(sentence_scores, sentence_info))\n",
    "    # arrs = np.split(arr, segment_splits)\n",
    "    # segment_info = [f(i) for i in arrs]\n",
    "\n",
    "    # I = np.searchsorted(segment_splits, sentence_splits) + 1\n",
    "    # sentence_info = np.split(segment_info, I)\n",
    "    # sentence_info = ['  '.join(i.tolist()) for i in sentence_info]\n",
    "    # for i, j in zip(sentence_scores, sentence_info):\n",
    "    #     print (i, j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a7cd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./test_data', 'w') as fd:\n",
    "\n",
    "    for n, (reviews, star, raw_review) in enumerate(\n",
    "        zip(test_data[\"ws\"], test_data['data'].stars, test_data['data'].reviews)):\n",
    "\n",
    "        e_star, sentence_info = check(reviews)\n",
    "        if e_star == star:\n",
    "            continue\n",
    "        info = '--- %d ---\\n' % n\n",
    "        info += raw_review + '\\n'\n",
    "        for i, j in sentence_info:\n",
    "            info += '%d => %s\\n' % (i, j)\n",
    "        info += 'estima star: %f\\n' % e_star\n",
    "        info += 'origin star: %f\\n\\n' % star\n",
    "        \n",
    "        # write into file\n",
    "        fd.write(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9d2791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A = []\n",
    "\n",
    "# for n, (reviews, star) in enumerate(zip(test_data[\"ws\"], test_data['data'].stars)):\n",
    "    \n",
    "#     # numerator\n",
    "#     scores = review_scores_based_on_sentence(reviews)\n",
    "    \n",
    "#     numerator = np.sum(scores)\n",
    "# #     print(numerator)\n",
    "# #     print(scores)\n",
    "#     denominator = len(np.nonzero(scores)[0])\n",
    "#     estimated_star = score_to_star(numerator, denominator)\n",
    "#     A.append(estimated_star)\n",
    "    \n",
    "#     if estimated_star != star:\n",
    "    \n",
    "#         print ('--- %d ---' % n)\n",
    "#         print (reviews)\n",
    "#         print (scores)\n",
    "#         print (estimated_star, star)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cab77a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['data'].reviews[66]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5b901c",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_review_scores(test_data['ws'][66], return_details=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c89461",
   "metadata": {},
   "outputs": [],
   "source": [
    "I = np.argsort(A)\n",
    "X = range(len(A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777afebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X, np.array(A)[I], X, test_data['data'].stars[I], '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23be86c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in os.listdir(path):\n",
    "    B = {}\n",
    "    if filename.endswith(\".pkl\"):\n",
    "        data = read_pickle(os.path.join(path, filename))\n",
    "        A = []\n",
    "        for n, (reviews, star) in enumerate(zip(data[\"ws\"], data['data'].stars)):\n",
    "            scores = review_scores_based_on_sentence(reviews)\n",
    "            numerator = np.sum(scores)\n",
    "            denominator = len(np.nonzero(scores)[0])\n",
    "            estimated_star = round(score_to_star(numerator, denominator), 2)\n",
    "            A.append(estimated_star)    \n",
    "        B[\"senti_star\"] = A\n",
    "        B[\"time\"] = data[\"data\"].datetime\n",
    "        B[\"username\"] = data[\"data\"].usernames\n",
    "        B[\"origin_star\"] = data[\"data\"].stars\n",
    "        B[\"review\"] = data[\"data\"].reviews\n",
    "        \n",
    "    df = pd.DataFrame.from_dict(B)\n",
    "    #df = df[[\"time\", \"username\", \"origin_star\", \"senti_star\", \"review\"]]\n",
    "    df.to_csv(os.path.join(\"../../Google_review_code/result/sentiments/Tainan\", filename +\".csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4626d9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in os.listdir(path):\n",
    "    if filename.endswith(\".pkl\"):\n",
    "        data = read_pickle(os.path.join(path, filename))\n",
    "        A = []\n",
    "        for n, (reviews, star) in enumerate(zip(data[\"ws\"], data['data'].stars)):\n",
    "            scores = review_scores_based_on_sentence(reviews)\n",
    "            numerator = np.sum(scores)\n",
    "            denominator = len(np.nonzero(scores)[0])\n",
    "            estimated_star = score_to_star(numerator, denominator)\n",
    "            A.append(estimated_star)\n",
    "            if estimated_star != star:\n",
    "                print ('--- %d ---' % n)\n",
    "                print (reviews)\n",
    "                print (scores)\n",
    "                print (estimated_star, star)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89afc978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# total = {}\n",
    "\n",
    "# for filename in os.listdir(path):\n",
    "#     three_type = {}\n",
    "#     pos = []\n",
    "#     neg = []\n",
    "#     neu = []\n",
    "#     if filename.endswith(\".pkl\"):\n",
    "#         data = read_pickle(os.path.join(path, filename))\n",
    "#         scores = [np.sum(review_scores_based_on_sentence(words)) for words in data['ws']]\n",
    "#         for score in scores:\n",
    "#             if score >= 1:\n",
    "#                 pos.append(score)\n",
    "#             if score <= -1:\n",
    "#                 neg.append(score)\n",
    "#             if score == 0:\n",
    "#                 neu.append(score)\n",
    "#         three_type[\"postive\"] = len(pos)\n",
    "#         three_type[\"negative\"] = len(neg)\n",
    "#         three_type[\"neutral\"] = len(neu)\n",
    "#         three_type[\"total\"] = len(data[\"ws\"])\n",
    "#     filename = filename.replace(\".pkl\", \"\")\n",
    "#     total[filename] = three_type\n",
    "\n",
    "# df = pd.DataFrame.from_dict(total)\n",
    "# df = df.T\n",
    "# df.to_csv(os.path.join(\"../../Google_review_code/result/sentiments/\", \"tainan_sentiment.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3183934",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../../cleaned_data/taichung/word_segments/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1c5d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in tqdm.tqdm(os.listdir(path)):\n",
    "    \n",
    "    if not filename.endswith(\".pkl\"):\n",
    "        continue\n",
    "    \n",
    "    data = read_pickle(os.path.join(path, filename))\n",
    "    senti_star = []\n",
    "    for n, (reviews, star) in enumerate(zip(data[\"ws\"], data['data'].stars)):\n",
    "        scores = review_scores_based_on_sentence(reviews)\n",
    "        numerator = np.sum(scores)\n",
    "        denominator = len(np.nonzero(scores)[0])\n",
    "        estimated_star = round(score_to_star(numerator, denominator), 2)\n",
    "        senti_star.append(estimated_star)   \n",
    "    \n",
    "    category = []\n",
    "    scores = [np.sum(review_scores_based_on_sentence(words)) for words in data['ws']]\n",
    "    for score in scores:\n",
    "        if score >= 1:\n",
    "            category.append('postive')\n",
    "        if score <= -1:\n",
    "            category.append('negative')\n",
    "        if score == 0:\n",
    "            category.append('neutral')\n",
    "    data = data['data']\n",
    "    total = list(zip(senti_star, category, data.strftime(), data.usernames, data.stars, data.reviews))\n",
    "    \n",
    "    df = pd.DataFrame(total, columns=['senti_star', 'category', 'time', 'username', 'origin_star', 'review'])\n",
    "    filename = filename.replace(\".pkl\", \"\")\n",
    "    f = os.path.join(\"../../Google_review_code/result/sentiments/taichung\", filename +\".csv\")\n",
    "#     print(f)\n",
    "    df.to_csv(f, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777d21ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# R = []\n",
    "\n",
    "# for filename in tqdm.tqdm(os.listdir(path)):\n",
    "#     if not filename.endswith(\".pkl\"):\n",
    "#         continue\n",
    "    \n",
    "#     data = read_pickle(os.path.join(path, filename))\n",
    "    \n",
    "#     A = []\n",
    "#     for n, (reviews, star) in enumerate(zip(data[\"ws\"], data['data'].stars)):\n",
    "#         scores = review_scores_based_on_sentence(reviews)\n",
    "#         numerator = np.sum(scores)\n",
    "#         denominator = len(np.nonzero(scores)[0])\n",
    "#         estimated_star = score_to_star(numerator, denominator)\n",
    "#         A.append(estimated_star)\n",
    "    \n",
    "#     name =  filename.replace('.pkl', '')\n",
    "#     senti_star = np.mean(A)\n",
    "#     orgin_star = np.mean(data[\"data\"].stars)\n",
    "#     R.append((name, senti_star, orgin_star))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d53b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A = pd.DataFrame(R, columns=['filename', 'senti_star', 'origin_star'])\n",
    "# A['senti_star'] = np.round(A['senti_star'], decimals=2)\n",
    "# A['origin_star'] = np.round(A['origin_star'], decimals=2)\n",
    "# A.to_csv(os.path.join(\"../../Google_review_code/result/sentiments/\", \"tainan_senti_star.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c97eec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
